# --- IMPORTS ---
import os
import tempfile
import requests
import traceback
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
from urllib.parse import urlparse

# --- WORKER IMPORTS ---
# Import the high-level functions and initializers from your worker script.
from worker import (
    load_environment_config,
    initialize_services,
    generate_document_id,
    process_and_index_pdf,
    process_and_index_docx,
    process_and_index_url,
    process_and_index_pptx,
    process_and_index_excel,
    process_and_index_image,
    process_and_index_zip,
    answer_queries,
    PROCESSED_DOCS_KEY, # Import for the Redis check
    RAG_CONFIG # Import the new config dictionary
)

# --- LIFESPAN MANAGEMENT & GLOBAL SERVICES ---
# This dictionary will hold our initialized services (clients, models, etc.)
services: Dict = {}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Handles startup and shutdown events for the FastAPI application.
    Initializes all necessary services on startup.
    """
    print("üöÄ API starting up...")
    print("Loading environment configuration...")
    config = load_environment_config()
    print("Initializing all services (DB, AI Models, etc.)...")
    services.update(initialize_services(config))
    print("‚úÖ Services initialized. API is ready.")
    yield
    print("üëã API shutting down...")
    services.clear()

app = FastAPI(
    title="DeepRead AI RAG API",
    description="API for the Bajaj HackRx 2025 RAG Pipeline.",
    version="1.1.0",
    lifespan=lifespan
)

# --- Pydantic Models ---

# This is the corrected code
class JobRequest(BaseModel):
    documents: str = Field(..., description="A URL pointing to the document (PDF, DOCX, or HTML page) to be processed.")
    questions: List[str] = Field(..., description="A list of questions to be answered based on the document.")
    semantic_weight: float = Field(RAG_CONFIG["semantic_weight"], description="The weight for semantic search results.")
    keyword_weight: float = Field(RAG_CONFIG["keyword_weight"], description="The weight for keyword search results.")
    k_semantic: int = Field(RAG_CONFIG["k_semantic"], description="Number of candidates for semantic search.")
    k_keyword: int = Field(RAG_CONFIG["k_keyword"], description="Number of candidates for keyword search.")
    k_rerank: int = Field(RAG_CONFIG["k_rerank"], description="Number of final contexts to use after reranking.")


# --- THIS IS THE NEWLY ADDED CLASS THAT FIXES THE ERROR ---
class JobResponse(BaseModel):
    """Defines the structure for the API's successful response."""
    job_id: str = Field(..., description="The unique identifier for the processing job, derived from the document source.")
    answers: List[str] = Field(..., description="A list of answers generated by the RAG pipeline, corresponding to the input questions.")


# --- API Endpoints ---

@app.post('/hackrx/run', response_model=JobResponse)
async def process_job(request: JobRequest):
    """
    Processes a document, answers questions about it, and returns the results.
    This endpoint handles document indexing (if new) and the full RAG pipeline.
    """
    document_url = request.documents
    document_id = generate_document_id(document_url)

    # --- STAGE 1: DOCUMENT PROCESSING & INDEXING ---
    # [cite_start]Check if the document has already been processed by checking its ID in Redis [cite: 2]
    if not services["redis_conn"].sismember(PROCESSED_DOCS_KEY, document_id): 
        temp_file_path = None
        try:
            url_path = urlparse(document_url).path
            file_extension = os.path.splitext(url_path)[-1].lower()

            if file_extension in [".pdf", ".docx",".pptx",".xlsx", ".xls",".jpg",".jpeg",".png"]:
                print(f"‚úÖ Detected '{file_extension}' from URL. Attempting to download file...")
                response = requests.get(document_url, timeout=30)
                response.raise_for_status()
                with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
                    temp_file.write(response.content)
                    temp_file_path = temp_file.name
                if file_extension == ".pdf": await process_and_index_pdf(temp_file_path, document_id, services)
                elif file_extension == ".docx": await process_and_index_docx(temp_file_path, document_id, services)
                elif file_extension == ".pptx": await process_and_index_pptx(temp_file_path, document_id, services)
                elif file_extension in ['.xlsx', '.xls']: await process_and_index_excel(temp_file_path, document_id, services)
                elif file_extension in ['.jpg', '.jpeg', '.png']: await process_and_index_image(temp_file_path, document_id, services)
            else:
                # [cite_start]If no common document extension is found, assume it's a webpage [cite: 2]
                print(f"‚ö†Ô∏è No specific file extension detected. Assuming it is a webpage to scrape...")
                await process_and_index_url(document_url, document_id, services)
        except Exception as e:
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Failed to process document: {str(e)}")
        finally:
            # [cite_start]Clean up the temporary file if one was created [cite: 2]
            if temp_file_path: os.unlink(temp_file_path)
    else:
        print(f"‚úÖ Document ID '{document_id}' found in cache. Skipping indexing.")

    # --- STAGE 2: QUESTION ANSWERING ---
    try:
        print(f"ü§ñ Starting Q&A pipeline for {len(request.questions)} questions...")
        # [cite_start]Call the main question-answering function from the worker module [cite: 2]
        answers = await answer_queries(
            queries=[(q, document_id) for q in request.questions],
            services=services,
            semantic_weight=request.semantic_weight,
            keyword_weight=request.keyword_weight,
            k_semantic=request.k_semantic,
            k_keyword=request.k_keyword,
            k_rerank=request.k_rerank
        )
        print("‚úÖ Q&A pipeline complete.")
        # [cite_start]Return the job ID and the generated answers [cite: 2]
        return {"job_id": document_id, "answers": answers}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to generate answers: {str(e)}")

@app.get('/')
def root():
    """A simple endpoint to confirm the API is live."""
    return {"message": "DeepRead AI RAG Worker API is live"}